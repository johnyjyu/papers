# papers
Papers read or to be read

Read: 

Attention is all you need

Bert: Pre-training of deep bidirectional transformers for language understanding

Deep learning based recommender system: A survey and new perspectives

Xlnet: Generalized autoregressive pretraining for language understanding

Transformer-xl: Attentive language models beyond a fixed-length context

One model to learn them all

MaskGAN: better text generation via filling in the_


To read: 

